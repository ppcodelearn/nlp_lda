{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Important Paragraph:\n",
      "Deep learning is a subset of machine learning.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'is_unprintable' from 'charset_normalizer.utils' (c:\\Users\\phili\\anaconda3\\envs\\nlp_venv\\Lib\\site-packages\\charset_normalizer\\utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpdfplumber\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpairwise\u001b[39;00m \u001b[39mimport\u001b[39;00m cosine_similarity\n",
      "File \u001b[1;32mc:\\Users\\phili\\anaconda3\\envs\\nlp_venv\\Lib\\site-packages\\pdfplumber\\__init__.py:11\u001b[0m\n\u001b[0;32m      1\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m__version__\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mutils\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mset_debug\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m ]\n\u001b[0;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpdfminer\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpdfminer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpdftypes\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_version\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__\n",
      "File \u001b[1;32mc:\\Users\\phili\\anaconda3\\envs\\nlp_venv\\Lib\\site-packages\\pdfminer\\pdftypes.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mccitt\u001b[39;00m \u001b[39mimport\u001b[39;00m ccittfaxdecode\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlzw\u001b[39;00m \u001b[39mimport\u001b[39;00m lzwdecode\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpsparser\u001b[39;00m \u001b[39mimport\u001b[39;00m LIT\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpsparser\u001b[39;00m \u001b[39mimport\u001b[39;00m PSException\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpsparser\u001b[39;00m \u001b[39mimport\u001b[39;00m PSObject\n",
      "File \u001b[1;32mc:\\Users\\phili\\anaconda3\\envs\\nlp_venv\\Lib\\site-packages\\pdfminer\\psparser.py:22\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     Any,\n\u001b[0;32m      9\u001b[0m     BinaryIO,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     Union,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m settings\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m choplist\n\u001b[0;32m     24\u001b[0m log \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mPSException\u001b[39;00m(\u001b[39mException\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\phili\\anaconda3\\envs\\nlp_venv\\Lib\\site-packages\\pdfminer\\utils.py:31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     29\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlayout\u001b[39;00m \u001b[39mimport\u001b[39;00m LTComponent\n\u001b[1;32m---> 31\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcharset_normalizer\u001b[39;00m  \u001b[39m# For str encoding detection\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39m# from sys import maxint as INF doesn't work anymore under Python3, but PDF\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m# still uses 32 bits ints\u001b[39;00m\n\u001b[0;32m     35\u001b[0m INF \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m<<\u001b[39m \u001b[39m31\u001b[39m) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\phili\\anaconda3\\envs\\nlp_venv\\Lib\\site-packages\\charset_normalizer\\__init__.py:24\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mCharset-Normalizer\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39m:license: MIT, see LICENSE for more details.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m from_bytes, from_fp, from_path, normalize\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlegacy\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     26\u001b[0m     CharsetDetector,\n\u001b[0;32m     27\u001b[0m     CharsetDoctor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     detect,\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m CharsetMatch, CharsetMatches\n",
      "File \u001b[1;32mc:\\Users\\phili\\anaconda3\\envs\\nlp_venv\\Lib\\site-packages\\charset_normalizer\\api.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     PathLike \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcd\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m     coherence_ratio,\n\u001b[0;32m     12\u001b[0m     encoding_languages,\n\u001b[0;32m     13\u001b[0m     mb_encoding_languages,\n\u001b[0;32m     14\u001b[0m     merge_coherence_ratios,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconstant\u001b[39;00m \u001b[39mimport\u001b[39;00m IANA_SUPPORTED, TOO_BIG_SEQUENCE, TOO_SMALL_SEQUENCE, TRACE\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmd\u001b[39;00m \u001b[39mimport\u001b[39;00m mess_ratio\n",
      "File \u001b[1;32mc:\\Users\\phili\\anaconda3\\envs\\nlp_venv\\Lib\\site-packages\\charset_normalizer\\cd.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39massets\u001b[39;00m \u001b[39mimport\u001b[39;00m FREQUENCIES\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconstant\u001b[39;00m \u001b[39mimport\u001b[39;00m KO_NAMES, LANGUAGE_SUPPORTED_COUNT, TOO_SMALL_SEQUENCE, ZH_NAMES\n\u001b[1;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmd\u001b[39;00m \u001b[39mimport\u001b[39;00m is_suspiciously_successive_range\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m CoherenceMatches\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     12\u001b[0m     is_accentuated,\n\u001b[0;32m     13\u001b[0m     is_latin,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     unicode_range,\n\u001b[0;32m     17\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\phili\\anaconda3\\envs\\nlp_venv\\Lib\\site-packages\\charset_normalizer\\md.py:10\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m List, Optional\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconstant\u001b[39;00m \u001b[39mimport\u001b[39;00m COMMON_SAFE_ASCII_CHARACTERS, UNICODE_SECONDARY_RANGE_KEYWORD\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     is_accentuated,\n\u001b[0;32m      7\u001b[0m     is_ascii,\n\u001b[0;32m      8\u001b[0m     is_case_variable,\n\u001b[0;32m      9\u001b[0m     is_cjk,\n\u001b[1;32m---> 10\u001b[0m     is_emoticon,\n\u001b[0;32m     11\u001b[0m     is_hangul,\n\u001b[0;32m     12\u001b[0m     is_hiragana,\n\u001b[0;32m     13\u001b[0m     is_katakana,\n\u001b[0;32m     14\u001b[0m     is_latin,\n\u001b[0;32m     15\u001b[0m     is_punctuation,\n\u001b[0;32m     16\u001b[0m     is_separator,\n\u001b[0;32m     17\u001b[0m     is_symbol,\n\u001b[0;32m     18\u001b[0m     is_thai,\n\u001b[0;32m     19\u001b[0m     remove_accent,\n\u001b[0;32m     20\u001b[0m     unicode_range,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mMessDetectorPlugin\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39m    Base abstract class used for mess detection plugins.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39m    All detectors MUST extend and implement given methods.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'is_unprintable' from 'charset_normalizer.utils' (c:\\Users\\phili\\anaconda3\\envs\\nlp_venv\\Lib\\site-packages\\charset_normalizer\\utils.py)"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Sample true value paragraph\n",
    "true_value_paragraph = \"\"\"\"In Figure 3, a detailed flowsheet of the one-step process is presented. This is an\n",
    "        adapted version from a concept reported in the literature [ 37– 39 ]. Feed CO2 is mixed\n",
    "        with a low-pressure recycle stream, and then compressed from 1 to 70 bar in a three-stage\n",
    "        process, including intermediate cooling (reducing compression work) and intermediate\n",
    "        phase separation (to remove condensed methanol and water from the recycle stream). The\n",
    "        resulting compressed stream is mixed with feed H2 (compressed from 30 to 70 bar in one\n",
    "        stage) and with a high pressure recycle stream. The mixed stream is preheated with the\n",
    "        product gas and enters the inner tubes of parallel reactor modules, with the temperature\n",
    "        being controlled by boiling water on the shell side.\n",
    "\"\"\"\n",
    "\n",
    "# PDF file path\n",
    "pdf_path = r\"C:\\Users\\phili\\techlabs\\nlp_delft\\paper\\Al_rabiah, Abdulrahman_Process_2022.pdf\"\n",
    "\n",
    "# Extract text from PDF\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# List of paragraphs extracted from PDF\n",
    "pdf_paragraphs = pdf_text.split('\\n\\n')  # You may need to adjust the paragraph delimiter based on the PDF content\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Create TF-IDF matrix for the PDF paragraphs\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(pdf_paragraphs)\n",
    "\n",
    "# Transform the true value paragraph using the same vectorizer\n",
    "true_value_tfidf = tfidf_vectorizer.transform([true_value_paragraph])\n",
    "\n",
    "# Calculate cosine similarity between true value paragraph and PDF paragraphs\n",
    "cosine_similarities = cosine_similarity(true_value_tfidf, tfidf_matrix)\n",
    "\n",
    "# Select the most important paragraphs from the PDF based on cosine similarity\n",
    "num_selected_paragraphs = 2\n",
    "important_paragraph_indices = cosine_similarities.argsort()[0][-num_selected_paragraphs:][::-1]\n",
    "\n",
    "# Print selected important paragraphs from the PDF\n",
    "print(\"Selected important paragraphs from the PDF:\")\n",
    "for idx in important_paragraph_indices:\n",
    "    print(pdf_paragraphs[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
