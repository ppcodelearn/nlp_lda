{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Paragraph: This is the first paragraph. It contains some relevant information. \n",
      "    The second paragraph has additional details. \n",
      "    Paragraph three is not very relevant. \n",
      "    The last paragraph summarizes the document's content. \n",
      "    \n",
      "    In recent years, natural language processing (NLP) has made significant advancements. \n",
      "    It enables machines to understand and process human language effectively. \n",
      "    NLP techniques are used in various applications, including chatbots, machine translation, \n",
      "    sentiment analysis, and text summarization.\n",
      "    \n",
      "    The field of NLP has seen tremendous growth due to the availability of large datasets and \n",
      "    powerful transformer-based models like BERT and GPT. These models can learn complex \n",
      "    linguistic patterns and have been fine-tuned for various tasks. As a result, NLP \n",
      "    applications have become more accurate and useful in real-world scenarios.\n",
      "    \n",
      "    TextRank is a popular algorithm for extractive text summarization and keyword extraction. \n",
      "    It is based on the PageRank algorithm used by Google for ranking web pages. TextRank \n",
      "    treats sentences or words in the text as nodes in a graph and uses their semantic similarity \n",
      "    to calculate scores. The algorithm then selects the most important sentences or keywords \n",
      "    based on these scores.\n",
      "    \n",
      "    However, when dealing with longer documents, traditional TextRank may not be sufficient. \n",
      "    In such cases, using a combination of semantic similarity and graph-based ranking can be \n",
      "    more effective. Additionally, transformer-based models can be used to calculate semantic \n",
      "    similarity, taking into account the context and meaning of the words and sentences in the \n",
      "    document and the query. This enables better selection of relevant paragraphs, sentences, \n",
      "    or keywords from the text.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import networkx as nx\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text into paragraphs\n",
    "    paragraphs = text.strip().split(\"\\n\\n\")\n",
    "\n",
    "    # Optionally, you can perform cleaning and normalization here\n",
    "\n",
    "    return paragraphs\n",
    "\n",
    "def textrank_best_paragraph(document, query):\n",
    "    # Preprocess the document and get the paragraphs\n",
    "    paragraphs = preprocess_text(document)\n",
    "\n",
    "    # Initialize the TF-IDF vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Compute the TF-IDF matrix for the paragraphs\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(paragraphs)\n",
    "\n",
    "    # Compute the cosine similarity between paragraphs and the query\n",
    "    query_vector = tfidf_vectorizer.transform([query])\n",
    "    paragraph_similarity = (tfidf_matrix * query_vector.T).A.flatten()\n",
    "\n",
    "    # Convert the similarity scores to a graph\n",
    "    graph = nx.Graph()\n",
    "    for i, sim_score in enumerate(paragraph_similarity):\n",
    "        graph.add_node(i, weight=sim_score)\n",
    "    for i in range(len(paragraphs)):\n",
    "        for j in range(i + 1, len(paragraphs)):\n",
    "            similarity = paragraph_similarity[i] * paragraph_similarity[j]\n",
    "            if similarity > 0:\n",
    "                graph.add_edge(i, j, weight=similarity)\n",
    "\n",
    "    # Rank the paragraphs using the PageRank algorithm\n",
    "    scores = nx.pagerank(graph, weight='weight')\n",
    "\n",
    "    # Get the index of the top-ranked paragraph\n",
    "    best_paragraph_index = max(scores, key=scores.get)\n",
    "\n",
    "    # Get the best paragraph\n",
    "    best_paragraph = paragraphs[best_paragraph_index]\n",
    "\n",
    "    return best_paragraph\n",
    "\n",
    "# Example usage with a longer document\n",
    "document = \"\"\"\n",
    "    This is the first paragraph. It contains some relevant information. \n",
    "    The second paragraph has additional details. \n",
    "    Paragraph three is not very relevant. \n",
    "    The last paragraph summarizes the document's content. \n",
    "    \n",
    "    In recent years, natural language processing (NLP) has made significant advancements. \n",
    "    It enables machines to understand and process human language effectively. \n",
    "    NLP techniques are used in various applications, including chatbots, machine translation, \n",
    "    sentiment analysis, and text summarization.\n",
    "    \n",
    "    The field of NLP has seen tremendous growth due to the availability of large datasets and \n",
    "    powerful transformer-based models like BERT and GPT. These models can learn complex \n",
    "    linguistic patterns and have been fine-tuned for various tasks. As a result, NLP \n",
    "    applications have become more accurate and useful in real-world scenarios.\n",
    "    \n",
    "    TextRank is a popular algorithm for extractive text summarization and keyword extraction. \n",
    "    It is based on the PageRank algorithm used by Google for ranking web pages. TextRank \n",
    "    treats sentences or words in the text as nodes in a graph and uses their semantic similarity \n",
    "    to calculate scores. The algorithm then selects the most important sentences or keywords \n",
    "    based on these scores.\n",
    "    \n",
    "    However, when dealing with longer documents, traditional TextRank may not be sufficient. \n",
    "    In such cases, using a combination of semantic similarity and graph-based ranking can be \n",
    "    more effective. Additionally, transformer-based models can be used to calculate semantic \n",
    "    similarity, taking into account the context and meaning of the words and sentences in the \n",
    "    document and the query. This enables better selection of relevant paragraphs, sentences, \n",
    "    or keywords from the text.\n",
    "\"\"\"\n",
    "\n",
    "query = \"natural language processing\"\n",
    "\n",
    "best_paragraph = textrank_best_paragraph(document, query)\n",
    "print(\"Best Paragraph:\", best_paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.doc.Doc' object has no attribute 'paragraphs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\phili\\techlabs\\nlp_delft\\par_an.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phili/techlabs/nlp_delft/par_an.ipynb#W1sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phili/techlabs/nlp_delft/par_an.ipynb#W1sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m  In recent years, natural language processing (NLP) has made significant advancements. \u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phili/techlabs/nlp_delft/par_an.ipynb#W1sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m  It enables machines to understand and process human language effectively. \u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phili/techlabs/nlp_delft/par_an.ipynb#W1sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m  or keywords from the text.\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phili/techlabs/nlp_delft/par_an.ipynb#W1sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phili/techlabs/nlp_delft/par_an.ipynb#W1sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGoogle\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/phili/techlabs/nlp_delft/par_an.ipynb#W1sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m best_paragraph \u001b[39m=\u001b[39m find_best_paragraph(text, query)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phili/techlabs/nlp_delft/par_an.ipynb#W1sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39mprint\u001b[39m(best_paragraph)\n",
      "\u001b[1;32mc:\\Users\\phili\\techlabs\\nlp_delft\\par_an.ipynb Cell 2\u001b[0m in \u001b[0;36mfind_best_paragraph\u001b[1;34m(text, query)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phili/techlabs/nlp_delft/par_an.ipynb#W1sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Create a vector representation of each paragraph.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phili/techlabs/nlp_delft/par_an.ipynb#W1sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m paragraph_vectors \u001b[39m=\u001b[39m []\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/phili/techlabs/nlp_delft/par_an.ipynb#W1sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m paragraph \u001b[39min\u001b[39;00m doc\u001b[39m.\u001b[39;49mparagraphs:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phili/techlabs/nlp_delft/par_an.ipynb#W1sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m   words \u001b[39m=\u001b[39m [token\u001b[39m.\u001b[39mlemma_ \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m paragraph]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phili/techlabs/nlp_delft/par_an.ipynb#W1sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m   word_vectors \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([nlp\u001b[39m.\u001b[39mvocab[word]\u001b[39m.\u001b[39mvector \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'spacy.tokens.doc.Doc' object has no attribute 'paragraphs'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "def find_best_paragraph(text, query):\n",
    "  \"\"\"Finds the best paragraph in a text given a query.\n",
    "\n",
    "  Args:\n",
    "    text: The text to search.\n",
    "    query: The query to search for.\n",
    "\n",
    "  Returns:\n",
    "    The paragraph with the highest similarity to the query.\n",
    "  \"\"\"\n",
    "\n",
    "  # Preprocess the text.\n",
    "  nlp = spacy.load(\"en_core_web_sm\")\n",
    "  doc = nlp(text)\n",
    "\n",
    "  # Create a vector representation of each paragraph.\n",
    "  paragraph_vectors = []\n",
    "  for paragraph in doc.paragraphs:\n",
    "    words = [token.lemma_ for token in paragraph]\n",
    "    word_vectors = np.array([nlp.vocab[word].vector for word in words])\n",
    "    paragraph_vectors.append(np.mean(word_vectors, axis=0))\n",
    "\n",
    "  # Calculate the similarity between each paragraph and the query.\n",
    "  similarities = np.dot(paragraph_vectors, query)\n",
    "\n",
    "  # Rank the paragraphs based on their similarity to the query.\n",
    "  best_paragraph = doc.paragraphs[np.argmax(similarities)]\n",
    "\n",
    "  return best_paragraph\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  text = \"\"\"\n",
    "    In recent years, natural language processing (NLP) has made significant advancements. \n",
    "    It enables machines to understand and process human language effectively. \n",
    "    NLP techniques are used in various applications, including chatbots, machine translation, \n",
    "    sentiment analysis, and text summarization.\n",
    "    \n",
    "    The field of NLP has seen tremendous growth due to the availability of large datasets and \n",
    "    powerful transformer-based models like BERT and GPT. These models can learn complex \n",
    "    linguistic patterns and have been fine-tuned for various tasks. As a result, NLP \n",
    "    applications have become more accurate and useful in real-world scenarios.\n",
    "    \n",
    "    TextRank is a popular algorithm for extractive text summarization and keyword extraction. \n",
    "    It is based on the PageRank algorithm used by Google for ranking web pages. TextRank \n",
    "    treats sentences or words in the text as nodes in a graph and uses their semantic similarity \n",
    "    to calculate scores. The algorithm then selects the most important sentences or keywords \n",
    "    based on these scores.\n",
    "    \n",
    "    However, when dealing with longer documents, traditional TextRank may not be sufficient. \n",
    "    In such cases, using a combination of semantic similarity and graph-based ranking can be \n",
    "    more effective. Additionally, transformer-based models can be used to calculate semantic \n",
    "    similarity, taking into account the context and meaning of the words and sentences in the \n",
    "    document and the query. This enables better selection of relevant paragraphs, sentences, \n",
    "    or keywords from the text.\n",
    "  \"\"\"\n",
    "\n",
    "  query = \"Google\"\n",
    "\n",
    "  best_paragraph = find_best_paragraph(text, query)\n",
    "\n",
    "  print(best_paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'Introduction', 'score': 0.24966977536678314}, {'label': 'Process', 'score': 0.24896663427352905}, {'label': 'Result', 'score': 0.24797721207141876}, {'label': 'literature', 'score': 0.2533864378929138}]\n"
     ]
    }
   ],
   "source": [
    "def zero_shot_text_classification(text, candidate_labels):\n",
    "    # Load the pre-trained Roberta model and tokenizer\n",
    "    model_name = \"roberta-base\"\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    model = RobertaModel.from_pretrained(model_name)\n",
    "\n",
    "    # Encode the input text and candidate labels\n",
    "    text_encoding = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    label_encodings = tokenizer(candidate_labels, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Get the embeddings for the text and candidate labels\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = model(**text_encoding).last_hidden_state.mean(dim=1)\n",
    "        label_embeddings = model(**label_encodings).last_hidden_state.mean(dim=1)\n",
    "\n",
    "    # Calculate cosine similarities between the text and candidate labels\n",
    "    similarities = cosine_similarity(text_embeddings, label_embeddings)\n",
    "\n",
    "    # Convert the similarities to probabilities\n",
    "    probabilities = torch.softmax(torch.tensor(similarities), dim=1)\n",
    "\n",
    "    # Prepare the results\n",
    "    results = []\n",
    "    for i, label in enumerate(candidate_labels):\n",
    "        results.append({\"label\": label, \"score\": probabilities[0, i].item()})\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "input_text = \"\"\"\n",
    "Developed process flow diagram (PFD) of the MIBK process. \n",
    "The fresh acetone feed (stream 1) enters the process at 70 °C and 1.8 atm in a liquid \n",
    "phase and is mixed with the recycled acetone (stream 14), which enters at 74 °C and 1.8 \n",
    "atm. Before entering the reactor R-101, the mixed stream is heated to 300 °C. The hydrogen \n",
    "feed is mixed with recycled hydrogen (stream 11) before being heated to 300 °C and fed \n",
    "into reactor R-101. Both reactor feeds pass through valves, V-101 and V-102, to reduce the \n",
    "pressure in the reactor. The hydrogen–acetone molar feed ratio is maintained at 2:1, as \n",
    "recommended [40,41]. The reaction proceeds isothermally in the gas phase at 300 °C and \n",
    "1 atm in a fixed bed catalytic reactor (R-101). The conversion of acetone is 66%, with a \n",
    "selectivity of 69.4% to methyl isobutyl ketone (MIBK). Other products, such as isopropa-\n",
    "nol (IPA) and diiasobutyl ketone (DIBK), are also produced. The selectivity of each of \n",
    "these substances was determined based on the experimental work, which is shown in Ta-\n",
    "ble 1. The reactor effluent will then be compressed to 6.5 atm to compensate for pressure \n",
    "losses through the pipelines and to enhance the separation of hydrogen without major \n",
    "product losses from the process stream at low pressure and high temperature, preventing \n",
    "the use of cryogenic conditions. MIBK losses at 1 atm are approximately 6 kmol/h. Conse-\n",
    "quently, compression is mandatory to avoid cryogenic conditions. At 6.5 atm, the losses \n",
    "drop to about 0.7 kmol/h of MIBK. After compression, the stream is cooled to 35 °C in E-\n",
    "Figure 1. Developed process ﬂow diagram (PFD) of the MIBK process.\n",
    "\"\"\"\n",
    "\n",
    "candidate_labels = [\"Introduction\", \"Process\", \"Result\",\"literature\"]\n",
    "\n",
    "classification_results = zero_shot_text_classification(input_text, candidate_labels)\n",
    "print(classification_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
